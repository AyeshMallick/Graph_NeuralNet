{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OtR6riRZ_eOs",
        "outputId": "27411b6f-913d-4f6f-ec1e-d017cbb88f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2by_BLQ-uzV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import InMemoryDataset, HeteroData\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Dataset Class\n",
        "class EventUserDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super(EventUserDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['categories.csv', 'events.csv', 'users.csv', 'user-edges.csv', 'user-event.csv', 'user-category.csv']\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        raw_dir = self.raw_dir\n",
        "        # Load CSVs\n",
        "        categories = pd.read_csv(os.path.join(raw_dir, 'categories.csv'))\n",
        "        events = pd.read_csv(os.path.join(raw_dir, 'events.csv'))\n",
        "        users = pd.read_csv(os.path.join(raw_dir, 'users.csv'))\n",
        "        user_edges = pd.read_csv(os.path.join(raw_dir, 'user-edges.csv'))\n",
        "        user_event = pd.read_csv(os.path.join(raw_dir, 'user-event.csv'))\n",
        "        user_category = pd.read_csv(os.path.join(raw_dir, 'user-category.csv'))\n",
        "\n",
        "        data = HeteroData()\n",
        "\n",
        "        # Create mappings for node indices\n",
        "        user_ids = users['user_id'].unique()\n",
        "        event_ids = events['event_id'].unique()\n",
        "        category_ids = categories['category_id'].unique()\n",
        "        user_id_mapping = {uid: i for i, uid in enumerate(user_ids)}\n",
        "        event_id_mapping = {eid: i for i, eid in enumerate(event_ids)}\n",
        "        category_id_mapping = {cid: i for i, cid in enumerate(category_ids)}\n",
        "\n",
        "        # For simplicity, assign random features (dimension 16) for each node type.\n",
        "        feat_dim = 16\n",
        "        num_users = len(user_ids)\n",
        "        num_events = len(event_ids)\n",
        "        num_categories = len(category_ids)\n",
        "        data['user'].x = torch.randn(num_users, feat_dim)\n",
        "        data['event'].x = torch.randn(num_events, feat_dim)\n",
        "        data['category'].x = torch.randn(num_categories, feat_dim)\n",
        "\n",
        "        # Build edges:\n",
        "\n",
        "        # (a) User-User: from user_edges.csv\n",
        "        src_uu = user_edges['user1'].map(user_id_mapping).values\n",
        "        dst_uu = user_edges['user2'].map(user_id_mapping).values\n",
        "        weight_uu = torch.tensor(user_edges['weight'].values, dtype=torch.float)\n",
        "        edge_index_uu = torch.tensor([src_uu, dst_uu], dtype=torch.long)\n",
        "        data['user', 'follows', 'user'].edge_index = edge_index_uu\n",
        "        data['user', 'follows', 'user'].edge_attr = weight_uu.unsqueeze(1)\n",
        "        # Reverse edge\n",
        "        edge_index_uu_rev = torch.tensor([dst_uu, src_uu], dtype=torch.long)\n",
        "        data['user', 'follows_rev', 'user'].edge_index = edge_index_uu_rev\n",
        "        data['user', 'follows_rev', 'user'].edge_attr = weight_uu.unsqueeze(1)\n",
        "\n",
        "        # (b) User-Event: from user-event.csv (user attended event)\n",
        "        src_ue = user_event['user_id'].map(user_id_mapping).values\n",
        "        dst_ue = user_event['event_id'].map(event_id_mapping).values\n",
        "        edge_index_ue = torch.tensor([src_ue, dst_ue], dtype=torch.long)\n",
        "        data['user', 'attends', 'event'].edge_index = edge_index_ue\n",
        "        # Reverse edge: event attended by user\n",
        "        edge_index_eu = torch.tensor([dst_ue, src_ue], dtype=torch.long)\n",
        "        data['event', 'attended_by', 'user'].edge_index = edge_index_eu\n",
        "\n",
        "        # (c) User-Category: from user-category.csv (user interests)\n",
        "        src_uc = user_category['user_id'].map(user_id_mapping).values\n",
        "        dst_uc = user_category['category_id'].map(category_id_mapping).values\n",
        "        weight_uc = torch.tensor(user_category['weight'].values, dtype=torch.float)\n",
        "        edge_index_uc = torch.tensor([src_uc, dst_uc], dtype=torch.long)\n",
        "        data['user', 'interested_in', 'category'].edge_index = edge_index_uc\n",
        "        data['user', 'interested_in', 'category'].edge_attr = weight_uc.unsqueeze(1)\n",
        "        # Reverse edge: category to user\n",
        "        edge_index_cu = torch.tensor([dst_uc, src_uc], dtype=torch.long)\n",
        "        data['category', 'has_interest_from', 'user'].edge_index = edge_index_cu\n",
        "        data['category', 'has_interest_from', 'user'].edge_attr = weight_uc.unsqueeze(1)\n",
        "\n",
        "        # (d) Event-Category: from events.csv (each event belongs to a category)\n",
        "        event_cat = events['category_id'].map(lambda x: category_id_mapping[x]).values\n",
        "        event_idx = events['event_id'].map(event_id_mapping).values\n",
        "        edge_index_ec = torch.tensor([event_idx, event_cat], dtype=torch.long)\n",
        "        data['event', 'belongs_to', 'category'].edge_index = edge_index_ec\n",
        "        # Reverse edge: category has event\n",
        "        edge_index_ce = torch.tensor([event_cat, event_idx], dtype=torch.long)\n",
        "        data['category', 'has_event', 'event'].edge_index = edge_index_ce\n",
        "\n",
        "        positive_pairs = []\n",
        "        for _, row in user_event.iterrows():\n",
        "            uid = user_id_mapping[row['user_id']]\n",
        "            eid = event_id_mapping[row['event_id']]\n",
        "            positive_pairs.append([uid, eid, 1])\n",
        "        positive_pairs = np.array(positive_pairs)\n",
        "\n",
        "        negative_pairs = []\n",
        "        user_event_dict = {}\n",
        "        for _, row in user_event.iterrows():\n",
        "            uid = user_id_mapping[row['user_id']]\n",
        "            eid = event_id_mapping[row['event_id']]\n",
        "            user_event_dict.setdefault(uid, set()).add(eid)\n",
        "        for uid in range(num_users):\n",
        "            num_pos = len(user_event_dict.get(uid, []))\n",
        "            if num_pos == 0:\n",
        "                continue\n",
        "            # All events that the user did NOT attend:\n",
        "            possible_events = set(range(num_events)) - user_event_dict.get(uid, set())\n",
        "            if not possible_events:\n",
        "                continue\n",
        "            sampled = np.random.choice(list(possible_events), size=num_pos, replace=len(possible_events) < num_pos)\n",
        "            for eid in sampled:\n",
        "                negative_pairs.append([uid, int(eid), 0])\n",
        "        negative_pairs = np.array(negative_pairs)\n",
        "\n",
        "        # Combine positive and negative pairs, then shuffle and split into train/test.\n",
        "        all_pairs = np.concatenate([positive_pairs, negative_pairs], axis=0)\n",
        "        np.random.shuffle(all_pairs)\n",
        "        all_pairs = torch.tensor(all_pairs, dtype=torch.long)\n",
        "        train_pairs, test_pairs = train_test_split(all_pairs, test_size=0.2, random_state=42)\n",
        "        # Save pairs inside the data object\n",
        "        data['train_pairs'] = train_pairs\n",
        "        data['test_pairs'] = test_pairs\n",
        "\n",
        "        # Save the processed data object.\n",
        "        torch.save(self.collate([data]), self.processed_paths[0])\n",
        "        return self.collate([data])"
      ],
      "metadata": {
        "id": "gZ58nHIg_Ht6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Heterogeneous Model Development:\n",
        "class BaseGNN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers=2):\n",
        "        super(BaseGNN, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        # First layer: in_channels -> hidden_channels\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        # Additional layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "607FRz4I_Hr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PairClassifier: combines user & event embeddings for binary classification.\n",
        "class PairClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels):\n",
        "        super(PairClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(2 * in_channels, hidden_channels)\n",
        "        self.fc2 = nn.Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, user_emb, event_emb):\n",
        "        # Concatenate embeddings\n",
        "        x = torch.cat([user_emb, event_emb], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OXBSB-83_Hpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Evaluation\n",
        "def train(model, classifier, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass through the hetero GNN.\n",
        "    out_dict = model(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "    # Get training pairs (each row: [user_idx, event_idx, label])\n",
        "    train_pairs = data['train_pairs']\n",
        "    user_indices = train_pairs[:, 0]\n",
        "    event_indices = train_pairs[:, 1]\n",
        "    labels = train_pairs[:, 2].float()\n",
        "\n",
        "    user_emb = out_dict['user'][user_indices]\n",
        "    event_emb = out_dict['event'][event_indices]\n",
        "    logits = classifier(user_emb, event_emb).squeeze()\n",
        "\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "bcUxbFh0_Hna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(model, classifier, data):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    out_dict = model(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "    test_pairs = data['test_pairs']\n",
        "    user_indices = test_pairs[:, 0]\n",
        "    event_indices = test_pairs[:, 1]\n",
        "    labels = test_pairs[:, 2].float()\n",
        "\n",
        "    user_emb = out_dict['user'][user_indices]\n",
        "    event_emb = out_dict['event'][event_indices]\n",
        "    logits = classifier(user_emb, event_emb).squeeze()\n",
        "    preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "    accuracy = (preds == labels).sum().item() / labels.size(0)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "9qf9-ab7_HlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function: load data, build model, train, and evaluate.\n",
        "def main():\n",
        "    # The dataset is expected in the folder \"data/EventUser/raw\" (for raw CSV files)\n",
        "    dataset = EventUserDataset(root='data')\n",
        "    data = dataset[0]\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create the heterogeneous GNN.\n",
        "    # First, build the base GNN which works on homogeneous graphs.\n",
        "    base_gnn = BaseGNN(in_channels=16, hidden_channels=32, num_layers=2)\n",
        "    # Convert the base GNN to a heterogeneous model using the graph metadata.\n",
        "    model = to_hetero(base_gnn, metadata=data.metadata(), aggr='sum').to(device)\n",
        "\n",
        "    # Create the classifier for user-event pair prediction.\n",
        "    classifier = PairClassifier(in_channels=32, hidden_channels=16).to(device)\n",
        "\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=0.005)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    epochs = 50\n",
        "    for epoch in range(epochs):\n",
        "        loss = train(model, classifier, data, optimizer, criterion)\n",
        "        if epoch % 5 == 0:\n",
        "            acc = test(model, classifier, data)\n",
        "            print(f'Epoch {epoch:03d} | Loss: {loss:.4f} | Test Accuracy: {acc:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC9BbL9e_Hi-",
        "outputId": "96103070-ecf2-4f92-ba64-43cc183d8825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-119487b8d481>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000 | Loss: 0.7039 | Test Accuracy: 0.5198\n",
            "Epoch 005 | Loss: 0.6724 | Test Accuracy: 0.5780\n",
            "Epoch 010 | Loss: 0.6211 | Test Accuracy: 0.6639\n",
            "Epoch 015 | Loss: 0.5484 | Test Accuracy: 0.7344\n",
            "Epoch 020 | Loss: 0.4792 | Test Accuracy: 0.7859\n",
            "Epoch 025 | Loss: 0.4316 | Test Accuracy: 0.8105\n",
            "Epoch 030 | Loss: 0.4093 | Test Accuracy: 0.8232\n",
            "Epoch 035 | Loss: 0.3848 | Test Accuracy: 0.8380\n",
            "Epoch 040 | Loss: 0.3700 | Test Accuracy: 0.8458\n",
            "Epoch 045 | Loss: 0.3545 | Test Accuracy: 0.8532\n"
          ]
        }
      ]
    }
  ]
}